{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc -q -O \"ground_truth_cause.csv\" \"https://raw.githubusercontent.com/CrowdTruth/Medical-Relation-Extraction/master/ground_truth_cause.csv\"\n",
    "!wget -nc -q -O \"ground_truth_treat.csv\" \"https://raw.githubusercontent.com/CrowdTruth/Medical-Relation-Extraction/master/ground_truth_treat.csv\"\n",
    "!wget -nc -q -O \"ground_truth_cause.xlsx\" \"https://github.com/CrowdTruth/Medical-Relation-Extraction/blob/master/train_dev_test/ground_truth_cause.xlsx?raw=true\"\n",
    "!wget -nc -q -O \"ground_truth_treat.xlsx\" \"https://github.com/CrowdTruth/Medical-Relation-Extraction/blob/master/train_dev_test/ground_truth_treat.xlsx?raw=true\"\n",
    "!wget -nc -q -O \"food_disease_dataset.csv\" \"https://raw.githubusercontent.com/gjorgjinac/food-disease-dataset/main/food_disease_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Disease dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/Users/adamkovacs/data/food-disease-dataset/splits/cause_folds/fold0/train.csv\", sep=\",\", quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(\"/Users/adamkovacs/data/food-disease-dataset/splits/cause_folds/fold0/val.csv\", sep=\",\", quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_entities(df):\n",
    "    sen = re.sub(re.escape(df.term1), 'XXX', df.sentence, flags=re.IGNORECASE)\n",
    "    sen = re.sub(re.escape(df.term2), 'YYY', sen, flags=re.IGNORECASE)\n",
    "    return sen.encode('ascii', errors='ignore').decode('utf-8')\n",
    "    \n",
    "\n",
    "df_train['preprocessed_sen'] = df_train.apply(extract_entities, axis=1)\n",
    "df_train['treat_label'] = df_train.is_treat.replace({1: 'TREAT', 0: 'NOT'})\n",
    "df_train['cause_label'] = df_train.is_cause.replace({1: 'CAUSE', 0: 'NOT'})\n",
    "\n",
    "df_dev['preprocessed_sen'] = df_dev.apply(extract_entities, axis=1)\n",
    "df_dev['treat_label'] = df_dev.is_treat.replace({1: 'TREAT', 0: 'NOT'})\n",
    "df_dev['cause_label'] = df_dev.is_cause.replace({1: 'CAUSE', 0: 'NOT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xpotato.dataset.dataset import Dataset\n",
    "from xpotato.models.trainer import GraphTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting treat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows = df_train.iterrows()\n",
    "dev_rows = df_dev.iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 17:46:40 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | craft   |\n",
      "| pos       | craft   |\n",
      "| lemma     | craft   |\n",
      "| depparse  | craft   |\n",
      "=======================\n",
      "\n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | craft   |\n",
      "| pos       | craft   |\n",
      "| lemma     | craft   |\n",
      "| depparse  | craft   |\n",
      "=======================\n",
      "\n",
      "2021-10-08 17:46:40 INFO: Use device: cpu\n",
      "INFO:stanza:Use device: cpu\n",
      "2021-10-08 17:46:40 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2021-10-08 17:46:40 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "2021-10-08 17:46:41 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "2021-10-08 17:46:41 INFO: Loading: depparse\n",
      "INFO:stanza:Loading: depparse\n",
      "2021-10-08 17:46:41 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n",
      "WARNING:root:loading NLP cache from en_nlp_cache...\n",
      "WARNING:root:done!\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 492/492 [01:04<00:00,  7.67it/s]\n",
      "2021-10-08 17:47:47 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | craft   |\n",
      "| pos       | craft   |\n",
      "| lemma     | craft   |\n",
      "| depparse  | craft   |\n",
      "=======================\n",
      "\n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | craft   |\n",
      "| pos       | craft   |\n",
      "| lemma     | craft   |\n",
      "| depparse  | craft   |\n",
      "=======================\n",
      "\n",
      "2021-10-08 17:47:47 INFO: Use device: cpu\n",
      "INFO:stanza:Use device: cpu\n",
      "2021-10-08 17:47:47 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2021-10-08 17:47:47 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "2021-10-08 17:47:48 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "2021-10-08 17:47:48 INFO: Loading: depparse\n",
      "INFO:stanza:Loading: depparse\n",
      "2021-10-08 17:47:48 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n",
      "WARNING:root:loading NLP cache from en_nlp_cache...\n",
      "WARNING:root:done!\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 55/55 [00:07<00:00,  7.53it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sentences = [(row[1].preprocessed_sen, row[1].treat_label) for row in train_rows]\n",
    "dev_sentences = [(row[1].preprocessed_sen, row[1].treat_label) for row in dev_rows]\n",
    "\n",
    "train_dataset = Dataset(train_sentences, label_vocab={\"TREAT\":1, \"NOT\": 0}, lang='en_bio')\n",
    "train_dataset.set_graphs(train_dataset.parse_graphs(graph_format=\"ud\"))\n",
    "\n",
    "dev_dataset = Dataset(dev_sentences, label_vocab={\"TREAT\":1, \"NOT\": 0}, lang='en_bio')\n",
    "dev_dataset.set_graphs(dev_dataset.parse_graphs(graph_format=\"ud\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset.to_dataframe()\n",
    "dev_df = dev_dataset.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle(\"food_train_dataset_treat_ud.pickle\")\n",
    "dev_df.to_pickle(\"food_dev_dataset_treat_ud.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows = df_train.iterrows()\n",
    "dev_rows = df_dev.iterrows()\n",
    "\n",
    "train_sentences = [(row[1].preprocessed_sen, row[1].cause_label) for row in train_rows]\n",
    "dev_sentences = [(row[1].preprocessed_sen, row[1].cause_label) for row in dev_rows]\n",
    "\n",
    "train_dataset_cause = Dataset(train_sentences, label_vocab={\"CAUSE\":1, \"NOT\": 0})\n",
    "train_dataset_cause.set_graphs(train_dataset.graphs)\n",
    "\n",
    "dev_dataset_cause = Dataset(dev_sentences, label_vocab={\"CAUSE\":1, \"NOT\": 0})\n",
    "dev_dataset_cause.set_graphs(dev_dataset.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset.to_dataframe()\n",
    "dev_df = dev_dataset.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle(\"food_train_dataset_cause_fourlang.pickle\")\n",
    "dev_df.to_pickle(\"food_dev_dataset_cause_fourang.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
